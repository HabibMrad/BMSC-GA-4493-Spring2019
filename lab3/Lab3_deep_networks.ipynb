{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Deep Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "x1 = np.random.normal(0, 1, (num_samples,))\n",
    "x2 = np.random.normal(0, 1, (num_samples,))\n",
    "x3 = np.random.normal(0, 1, (num_samples,))\n",
    "\n",
    "y = 3 * x1 ** 3 + 6 * x2 ** 2 + x3 + 3\n",
    "X = np.stack((x1, x2, x3, np.ones((num_samples,))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =100, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "Let's first try to approximate this function with linear regression using gradient descent\n",
    "\n",
    "Recall that in linear regression:\n",
    "$$f(x) = X\\theta$$\n",
    "\n",
    "The $L_{2}$ lost function, given $n$ samples:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{n}\\|y - X\\theta\\|_{2}^{2}$$\n",
    "\n",
    "The gradient of the lost function (for back propagation):\n",
    "\n",
    "$$\\nabla J(\\theta) = \\frac{2}{m}X^{T}(X\\theta - y)$$\n",
    "\n",
    "We update parameter $\\theta$ by\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\times \\nabla J(\\theta)$$\n",
    "\n",
    "Where $\\alpha$ is the step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The square loss function\n",
    "def compute_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the average square loss for predicting y with X*theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_samples, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_samples)\n",
    "        theta - the parameter vector, 1D array of size (num_samples)\n",
    "\n",
    "    Returns:\n",
    "        loss - the average square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #Initialize the average square loss\n",
    "    \n",
    "    ################ Your code here ################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The gradient of the square loss function\n",
    "def compute_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the average square loss\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_samples, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_samples)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_samples)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    ################ Your code here ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(X, y, alpha=0.01, num_step=1000):\n",
    "    \"\"\"\n",
    "    Batch gradient descent to minimize the average square loss objective.\n",
    "\n",
    "    Args:\n",
    "        X - A tuple containing two elements (X_train, X_test), \n",
    "            each one of which is a numpy array of size (num_samples, num_features)\n",
    "        y - A tuple containing two elements (y_train, y_test),\n",
    "            each one of which is a numpy array of size (num_samples)\n",
    "        alpha - step size in gradient descent\n",
    "        num_step - number of steps to run\n",
    "\n",
    "    Returns:\n",
    "        best_theta - the set of parameters that achieves smallest loss on test data\n",
    "        train_loss_hist - the history of average square loss on training data, 1D numpy array, (num_step)\n",
    "        test_loss_hist - the history of average square loss on testing data, 1D numpy array, (num_step)\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    X_train, X_test = X\n",
    "    y_train, y_test = y\n",
    "    num_features = X_train.shape[1]\n",
    "    theta = np.zeros(num_features)\n",
    "    \n",
    "    # Track performance\n",
    "    best_test_loss = float('Inf')\n",
    "    best_theta = theta\n",
    "    test_loss_hist = np.zeros(num_step)\n",
    "    train_loss_hist = np.zeros(num_step)\n",
    "    \n",
    "    ################ Your code here ################\n",
    "    \n",
    "\n",
    "\n",
    "    return (best_theta, train_loss_hist, test_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_loss, test_loss = batch_grad_descent((X_train, X_test), (y_train, y_test))\n",
    "x = np.arange(train_loss.shape[0])\n",
    "plt.plot(x, train_loss, label=\"Train Loss\")\n",
    "plt.plot(x, test_loss, label=\"Test Loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Gradient Descent\n",
    "\n",
    "Batch gradient descent with very large training data can take a very long time to compute, since it requires looking at each training example to take a single gradient step. Hence in reality, we often use minibatch gradient descent. \n",
    "\n",
    "One pass through the training data is called an **epoch**. In each epoch, training data are divided into minibatches after **randomly shuffling**. During training, we sweep through the whole training set one minibatch at a time, and perform a parameter update using one minibatch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### Minibatch gradient descent\n",
    "def minibatch_grad_descent(X, y, batch_size=50, alpha=0.01, num_epoch=50):\n",
    "    \"\"\"\n",
    "    Minibatch gradient descent to minimize the average square loss objective.\n",
    "\n",
    "    Args:\n",
    "        X - A tuple containing two elements (X_train, X_test), \n",
    "            each one of which is a numpy array of size (num_samples, num_features)\n",
    "        y - A tuple containing two elements (y_train, y_test),\n",
    "            each one of which is a numpy array of size (num_samples)\n",
    "        batch_size - minibatch size\n",
    "        alpha - step size in gradient descent\n",
    "        num_step - number of epochs to go through the whole training set\n",
    "\n",
    "    Returns:\n",
    "        best_theta - the set of parameters that achieves smallest loss on test data\n",
    "        train_loss_hist - the history of average square loss on training data, 1D numpy array, (num_step)\n",
    "        test_loss_hist - the history of average square loss on testing data, 1D numpy array, (num_step)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialization\n",
    "    X_train, X_test = X\n",
    "    y_train, y_test = y\n",
    "    num_samples, num_features = X_train.shape\n",
    "    num_batches = int(num_samples / batch_size)\n",
    "    theta = np.ones(num_features) \n",
    "    order = np.arange(num_samples)\n",
    "    step = 0\n",
    "    \n",
    "    # Track performance\n",
    "    best_test_loss = float('Inf')\n",
    "    best_theta = theta\n",
    "    train_loss_hist = np.zeros((num_epoch, num_batches))\n",
    "    test_loss_hist = np.zeros((num_epoch, num_batches))\n",
    "    \n",
    "    np.random.seed(432)\n",
    "    ################ Your code here ################\n",
    "\n",
    "    \n",
    "    return (best_theta, train_loss_hist, test_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_loss, test_loss = minibatch_grad_descent((X_train, X_test), (y_train, y_test))\n",
    "x = np.arange(train_loss.shape[0])\n",
    "plt.plot(x, np.mean(train_loss, axis=1), label=\"Train Loss\")\n",
    "plt.plot(x, np.mean(test_loss, axis=1), label=\"Test Loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation\n",
    "\n",
    "How can you modify minibatch gradient descent to make it work for PyTorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Your code here ################\n",
    "# Define network\n",
    "\n",
    "\n",
    "# Training Loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
