{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab4: Introduction to Convolutional Layers\n",
    "\n",
    "Authors: Aakash Kaku, Lee Tanenbaum\n",
    "\n",
    "The goal of this lab is to understand how to train a convolutional neural network using PyTorch. A lot of starter code will be given to you, and the student is expected to build the network details.\n",
    "\n",
    "The dataset we will analyze will be a small section of the nih chest xrays dataset, found here: https://www.kaggle.com/nih-chest-xrays/sample . Please create a Kaggle account to download the data (or access in a class folder if we can get server use set up). The dataset has images of resolution 1024x1024, but to make it computationally easier we have reduce the dimensionality to 64x64.\n",
    "\n",
    "The task at hand is to treat the dataset as a binary/multiclass classification problem with image inputs. We propose to build a model that is a series of spatial convolutional layers, activation functions, and pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into the code, lets think for a bit about model selection. What are the necessary choices?\n",
    "\n",
    "Number of hidden layers?\n",
    "\n",
    "For each layer:\n",
    "\n",
    "    Number of filters?\n",
    "\n",
    "    Size of kernel?\n",
    "\n",
    "    Size of padding? (maybe (kernel - 1) / 2)\n",
    "    \n",
    "    Stride of layer?\n",
    "    \n",
    "    Activation after layer:\n",
    "    \n",
    "        Some type of relu, tanh, sigmoid?\n",
    "        \n",
    "        Maybe add Batch normalization before the activation function?\n",
    "    \n",
    "    Maybe a pooling layer instead of a convolutional layer to decrease spatial dimension?\n",
    "\n",
    "    Learning Rate?\n",
    "\n",
    "    Momentum parameters for optimizers such as ADAM?\n",
    "\n",
    "Other training techniques such as adding noise to input or hidden layers?\n",
    "\n",
    "Image specific techniques such as random rotations or blurring of the image?\n",
    "\n",
    "Other optional enhancements:\n",
    "\n",
    "    Let us know if you have any ideas, there are approximately infinite different enhancements that can be included to help these types of models learn\n",
    "    \n",
    "Before you start to write code, try to have a choice of these hyperparameters in mind so you can try to implement them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import common dependencies\n",
    "import torch\n",
    "import pandas as pd, numpy as np, matplotlib, matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "### First, read in the sample labels which we will use as a lookup table while loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv('sample_labels.csv').iloc[:, :2]\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df['Disease']=(label_df['Finding Labels'] != 'No Finding').astype(int)\n",
    "print(label_df.head())\n",
    "num_rows = 1000\n",
    "label_df = label_df.iloc[:num_rows,:]\n",
    "\n",
    "# define train, val and test idx\n",
    "idx = np.arange(num_rows)\n",
    "np.random.shuffle(idx)\n",
    "train_size = 600\n",
    "val_size = 200\n",
    "test_size = 200\n",
    "train_idx = idx[:train_size]\n",
    "val_idx = idx[train_size:train_size+val_size]\n",
    "test_idx = idx[train_size+val_size:]\n",
    "\n",
    "# get train, val and test dataframes\n",
    "train_df = label_df.iloc[train_idx,:]\n",
    "val_df = label_df.iloc[val_idx,:]\n",
    "test_df = label_df.iloc[test_idx,:]\n",
    "\n",
    "# save the dataframes\n",
    "train_df.to_csv('train.csv', index = False)\n",
    "val_df.to_csv('val.csv', index = False)\n",
    "test_df.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We build dataloader to efficiently load the images and possibly also do some data augmentation on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xray_dataset(Dataset):\n",
    "    '''X-ray Dataset'''\n",
    "    def __init__(self, df_path, train = False):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        file_name = self.df.iloc[idx,0]\n",
    "        label = self.df.iloc[idx,-1]\n",
    "        img = Image.open('./images/'+file_name)\n",
    "        img = img.resize((64,64))\n",
    "        \n",
    "        if self.train:\n",
    "            # rotate the image (data augementation)\n",
    "            # TO DO\n",
    "            #Step 1: generate a random number\n",
    "            #Step 2: Check if the random number is greater than some threshold (0.7)\n",
    "            #Step 3: If yes for Step 3, then generate a random rotation angle between -10 and 10 degrees\n",
    "            #Step 4: Rotate the image using the rotation angle (check the PIL library for the command to rotate imgs)\n",
    "                \n",
    "        img = np.asarray(img)\n",
    "        min_image = np.min(img)\n",
    "        max_image = np.max(img)\n",
    "        img = (img - min_image)/(max_image - min_image + 1e-4)\n",
    "        \n",
    "        img = torch.tensor(img).unsqueeze(0).float()\n",
    "        label = torch.tensor(label).long()\n",
    "        if img.dim() != 3:\n",
    "            img = img[:,:,:,0]\n",
    "        \n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = './train.csv'\n",
    "val_df_path = './val.csv'\n",
    "test_df_path = './test.csv'\n",
    "transformed_dataset = {'train': Xray_dataset(train_df_path, train = True),\n",
    "                       'validate':Xray_dataset(val_df_path),\n",
    "                       'test':Xray_dataset(test_df_path),\n",
    "                                          }\n",
    "bs = 16\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs,\n",
    "                        shuffle=True, num_workers=0) for x in ['train', 'validate','test']}\n",
    "data_sizes ={x: len(transformed_dataset[x]) for x in ['train', 'validate','test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = next(iter(dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = sample[0][1].squeeze().numpy()\n",
    "plt.imshow(sample_img, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs = 10, verbose = False):\n",
    "    acc_dict = {'train':[],'validate':[]}\n",
    "    loss_dict = {'train':[],'validate':[]}\n",
    "    best_acc = 0\n",
    "    phases = ['train','validate']\n",
    "    since = time.time()\n",
    "    for i in range(num_epochs):\n",
    "        print('Epoch: {}/{}'.format(i, num_epochs-1))\n",
    "        print('-'*10)\n",
    "        for p in phases:\n",
    "            running_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            if p == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "                \n",
    "            for data in dataloader[p]:\n",
    "                num_imgs = image.size()[0]\n",
    "                # TO DO\n",
    "                # Step 1: zero the grad\n",
    "                # Step 2: get the input and label data (make sure they are on the same device as the model)\n",
    "                # Step 3: get the output\n",
    "                # Step 4: Compute the loss\n",
    "                # Step 5: Get the pred values\n",
    "                # Step 6: compute the number of corrects for this batch and increment the running_correct\n",
    "                running_loss += loss.item()*num_imgs\n",
    "                running_total += num_imgs\n",
    "                if p == 'train':\n",
    "                    # TO DO\n",
    "                    # Step 1: Peform the backward\n",
    "                    # Step 2: Take a gradient step\n",
    "            epoch_acc = float(running_correct/running_total)\n",
    "            epoch_loss = float(running_loss/running_total)\n",
    "            if verbose or (i%10 == 0):\n",
    "                print('Phase:{}, epoch loss: {:.4f} Acc: {:.4f}'.format(p, epoch_loss, epoch_acc))\n",
    "            \n",
    "            acc_dict[p].append(epoch_acc)\n",
    "            loss_dict[p].append(epoch_loss)\n",
    "            if p == 'validate':\n",
    "                # TO DO\n",
    "                # Save the model's state dict as best_model_wts if epoch_acc is greater than best_acc\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, acc_dict, loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader,loss_fn, phase = 'validate'):\n",
    "    model.eval()\n",
    "    running_correct = 0\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    for data in dataloader[phase]:\n",
    "        num_imgs = image.size()[0]\n",
    "        # TO DO\n",
    "        # Step 1: get the input and label data (make sure they are on the same device as the model)\n",
    "        # Step 2: get the output\n",
    "        # Step 3: Compute the loss\n",
    "        # Step 4: Get the pred values\n",
    "        # Step 5: compute the number of corrects for this batch and increment the running_correct\n",
    "        running_loss += loss.item()*num_imgs\n",
    "        running_total += num_imgs\n",
    "    accuracy = float(running_correct/running_total)\n",
    "    loss = float(running_loss/running_total)\n",
    "    \n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets build some models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First some common functions that could be useful to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nn.LeakyReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nn.MaxPool2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nn.Conv2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_model(nn.Module):\n",
    "    def __init__(self, kernel_size = 3):\n",
    "        super(Conv_model,self).__init__()\n",
    "        # TO DO\n",
    "        # Step 1: Instantiate Layer 1:\n",
    "            # Conv layer: Convolution with input channel(i) = 1, output channels (o) = 16, padding (p) = 1,\\\n",
    "            #          stride (s) = 1, kernel_size (k) = kernel_size\n",
    "            # Relu\n",
    "            \n",
    "        # Step 2: Instantiate Layer 2:\n",
    "            # Conv layer: Convolution(i = previous output channels, o = 16, p = 1,s = 1, k = kernel_size)\n",
    "            # Relu\n",
    "            \n",
    "        # Step 3: Instantiate Layer 3:\n",
    "            # Maxpooling layer (k = kernel_size, s = 2)\n",
    "        \n",
    "        # Step 4: Instantiate Layer 4:\n",
    "            # Conv layer: Convolution(i = previous output channels, o = 32, p = 1,s = 2, k = kernel_size)\n",
    "            # Relu\n",
    "            \n",
    "        # Step 5: Instantiate Layer 5:\n",
    "            # Conv layer: Convolution(i = previous output channels, o = 32, p = 1,s = 2, k = kernel_size)\n",
    "            # Relu\n",
    "        \n",
    "        # Step 6: Instantiate Layer 6:\n",
    "            # Maxpooling layer (k = kernel_size, s = 2)\n",
    "        \n",
    "        # Step 7: Instantiate Layer 7:\n",
    "            # Conv layer: Convolution(i = previous output channels, o = 32, p = 1,s = 1, k = kernel_size)\n",
    "            # Relu\n",
    "        \n",
    "        # Step 8: Instantiate Layer 8:\n",
    "            # Conv layer: Convolution(i = previous output channels, o = 2, p = 1,s = 1, k = kernel_size) \n",
    "            \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # TO DO\n",
    "        # Write the foward pass\n",
    "        x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "    \n",
    "        return x.view(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Step 1: Call the model class. Make sure to send the model to the desired device \n",
    "# Step 2: instantiate the Adam optimizer\n",
    "# Step 3: Use the train_model function to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now lets evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Use the evaluate_model function to evaluate the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
