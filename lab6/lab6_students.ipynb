{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Regularization and Optimization\n",
    "\n",
    "Ren Yi, 3-28-2019\n",
    "\n",
    "The goal of this lab is to learn how to apply different regularization and optimization strategies in PyTorch using MNIST data.\n",
    "\n",
    "Here a imcomplete list of the techniques we've covered in class\n",
    "- Regularization\n",
    "    - L1/L2 regularization\n",
    "    - Data augmentation\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    - Early stopping\n",
    "- Optimization\n",
    "    - SGD (with momentum)\n",
    "    - Nesterov momentum\n",
    "    - AdaGrad\n",
    "    - RMSProp\n",
    "    - Adam\n",
    "    \n",
    "We will show you how some of these methods are used in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize necessary parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(717)\n",
    "\n",
    "seed = 345\n",
    "batch_size = 50\n",
    "test_batch_size = 50\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "n_feature = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "trainset = datasets.MNIST('data', train=True, download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "testset = datasets.MNIST('data', train=False, \n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show some images\n",
    "plt.figure()\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epoch=1, print_every=100):\n",
    "    train_losses =[]\n",
    "    test_losses =[]\n",
    "    start_time = time.time()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # Training step\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % print_every == 0:\n",
    "                print('Train set | epoch: {:3d} | {:6d}/{:6d} batches | Loss: {:6.4f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()))\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                train_loss /=len(train_loader)\n",
    "                train_losses.append(train_loss)\n",
    "\n",
    "        # testing step\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += loss_fn(output, target).item()\n",
    "                pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "                correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "            test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "            print('Test set | Average loss: {:6.4f} | Accuracy: {:4.2f}% | time elapse: {:>9}'.format(\n",
    "                test_loss, 100. * correct / len(test_loader.dataset), elapse))\n",
    "\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "def populate_result(dictionary, method, train_loss, val_loss):\n",
    "    dictionary[method] = {}\n",
    "    dictionary[method]['train_loss'] = np.array(train_loss)\n",
    "    dictionary[method]['val_loss'] = np.array(val_loss)\n",
    "    \n",
    "def plot_loss(result, loss='train_loss', ylim=None):\n",
    "    plt.plot(result['Baseline'][loss], label='Baseline')\n",
    "    for k in result.keys():\n",
    "        if k != 'Baseline':\n",
    "            plt.plot(result[k][loss], label=k)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_best_loss(result, loss='val_loss'):\n",
    "    labels = ['Baseline']\n",
    "    acc = [np.max(result['Baseline'][loss])]\n",
    "    for k in result.keys():\n",
    "        if k != 'Baseline':\n",
    "            labels.append(k)\n",
    "            acc.append(np.max(result[k][loss]))\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    plt.barh(x, acc)\n",
    "    plt.yticks(x, labels)\n",
    "    plt.xlabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline method\n",
    "\n",
    "We will later show how different optimization and regularization techniques can improve baseline model performance. But first,\n",
    "1. What's our baseline model architecture?\n",
    "2. What's the optimization method used to train the baseline model?\n",
    "3. How does this optimization method update its parameters.\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\eta \\nabla J(\\theta_{t})$$\n",
    "where $\\eta$ denotes the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        self.fc = nn.Sequential(nn.Linear(n_feature*4*4, 50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(50, 10))\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(input_size, n_feature, output_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "train_losses, test_losses = train(model, optimizer)\n",
    "populate_result(optim_results, 'Baseline', train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancier Optimization\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with momentum\n",
    "1. How does SGD with momentum update its parameters?\n",
    "$$v_{t+1} = \\rho v_{t} + \\nabla J(\\theta_{t})$$\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\eta v_{t+1}$$\n",
    "where $v$ and $\\rho$ denote velocity and momentum, respectively.\n",
    "2. Check out the documentation for SGD in PyTorch and complete the code below (Set momentum=0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(input_size, n_feature, output_size).to(device)\n",
    "############ Your code below ############\n",
    "\n",
    "############       End       ############\n",
    "train_losses, test_losses = train(model, optimizer)\n",
    "populate_result(optim_results, 'SGD_momentum', train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov momentum\n",
    "\n",
    "Make a minor change in the above code to apply Nesterov momentum (Set momentum=0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(input_size, n_feature, output_size).to(device)\n",
    "############ Your code below ############\n",
    "\n",
    "############       End       ############\n",
    "train_losses, test_losses = train(model, optimizer)\n",
    "populate_result(optim_results, 'Nesterov_momentum', train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "1. What's the motivation of AdaGrad?\n",
    "2. Check out the documentation of AdaGrad in PyTorch and complete the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(input_size, n_feature, output_size).to(device)\n",
    "############ Your code below ############\n",
    "\n",
    "############       End       ############\n",
    "train_losses, test_losses = train(model, optimizer)\n",
    "populate_result(optim_results, 'AdaGrad', train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "\n",
    "Complete the code below for RMSprop. Set smoothing constant to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(input_size, n_feature, output_size).to(device)\n",
    "############ Your code below ############\n",
    "\n",
    "############       End       ############\n",
    "train_losses, test_losses = train(model, optimizer)\n",
    "populate_result(optim_results, 'RMSprop', train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(input_size, n_feature, output_size).to(device)\n",
    "############ Your code below ############\n",
    "\n",
    "############       End       ############\n",
    "train_losses, test_losses = train(model, optimizer)\n",
    "populate_result(optim_results, 'Adam', train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization methods training results\n",
    "plot_loss(optim_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization methods validation results\n",
    "plot_best_loss(optim_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to see effects of regularization on validation set, we need to make some slight modification\n",
    "\n",
    "## Smaller training set\n",
    "trainset.train_data=trainset.train_data[:6000]\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## Longer training epochs\n",
    "train_epoches = 100\n",
    "print_every = 500\n",
    "reg_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(input_size, n_feature, output_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "train_losses, test_losses = train(model, optimizer, num_epoch=train_epoches, print_every=print_every)\n",
    "populate_result(reg_results, 'Baseline', train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1/L2 regularization\n",
    "\n",
    "L2 regularization is included in most optimizers in PyTorch and can be controlled with the __weight_decay__ parameter.\n",
    "For L1 regularization, check out this post: https://discuss.pytorch.org/t/simple-l2-regularization/139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Layer\n",
    "\n",
    "1. How does Dropout introduce regularization effect?\n",
    "2. Check out documentations for __nn.Dropout()__. Modify the Baseline model and add dropout layer to the fully connected layers.\n",
    "3. Optionally, you may also check out documentations for __nn.Dropout2d()__ to learn how to add Dropout layer to convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutNet(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(DropoutNet, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        self.fc = nn.Sequential(nn.Linear(n_feature*4*4, 50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(50, 10))\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = DropoutNet(input_size, n_feature, output_size, dropout_rate=0.5).to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "train_losses, test_losses = train(model, optimizer, num_epoch=train_epoches, print_every=print_every)\n",
    "populate_result(reg_results, 'Dropout', train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "1. What's the advantage of using batch normalization?\n",
    "2. Batch normalization also act as a form of regularization, why?\n",
    "3. Modify the Baseline model and implement batch normalization in __BatchnormNet__. Think about where you may want to insert the batch normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchnormNet(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(BatchnormNet, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, n_feature, kernel_size=5),\n",
    "                                   nn.MaxPool2d(2),\n",
    "                                   nn.ReLU())\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "                                   nn.MaxPool2d(2),\n",
    "                                   nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.Linear(n_feature*4*4, 50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(50, 10))\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = BatchnormNet(input_size, n_feature, output_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "train_losses, test_losses = train(model, optimizer, num_epoch=train_epoches, print_every=print_every)\n",
    "populate_result(reg_results, 'Batchnorm', train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(reg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(reg_results, loss = 'val_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: parameter tunning\n",
    "\n",
    "We've introduced multiple regularization and optimization techniques to improve your model. How can you combine these techniques and perform grid search to find out a set of parameters that maximize your model performance on validation set? Are there other model architectures you'd like to try?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
