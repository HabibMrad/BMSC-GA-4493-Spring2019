{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 7: RNNs & Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Ren Yi\"\n",
    "__version__ = \"BMSC-GA 4493/BMIN-GA 3007, NYU, Spring 2019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Goal:\n",
    "- Understand the mechanics of RNNs in Pytorch\n",
    "- Train RNN based neural networks on text data\n",
    "- Basics of word embedding and how to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Download the two files in the data folder [here](https://drive.google.com/drive/folders/1KBUyfU87zz8eOZwr2ifDi2Z4LBHlSZ28?usp=sharing). Save the folder in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the first part, we will be using the [First GOP Debate Twitter Sentiment dataset](https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras/data), which contains Tweets after the first GOP debate and their sentiments (among other stuff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>candidate</th>\n",
       "      <th>candidate_confidence</th>\n",
       "      <th>relevant_yn</th>\n",
       "      <th>relevant_yn_confidence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>subject_matter</th>\n",
       "      <th>subject_matter_confidence</th>\n",
       "      <th>candidate_gold</th>\n",
       "      <th>...</th>\n",
       "      <th>relevant_yn_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sentiment_gold</th>\n",
       "      <th>subject_matter_gold</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697200650592256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199560069120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199312482304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.7039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697197118861312</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.7045</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697196967903232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               candidate  candidate_confidence relevant_yn  \\\n",
       "0   1  No candidate mentioned                   1.0         yes   \n",
       "1   2            Scott Walker                   1.0         yes   \n",
       "2   3  No candidate mentioned                   1.0         yes   \n",
       "3   4  No candidate mentioned                   1.0         yes   \n",
       "4   5            Donald Trump                   1.0         yes   \n",
       "\n",
       "   relevant_yn_confidence sentiment  sentiment_confidence     subject_matter  \\\n",
       "0                     1.0   Neutral                0.6578  None of the above   \n",
       "1                     1.0  Positive                0.6333  None of the above   \n",
       "2                     1.0   Neutral                0.6629  None of the above   \n",
       "3                     1.0  Positive                1.0000  None of the above   \n",
       "4                     1.0  Positive                0.7045  None of the above   \n",
       "\n",
       "   subject_matter_confidence candidate_gold             ...              \\\n",
       "0                     1.0000            NaN             ...               \n",
       "1                     1.0000            NaN             ...               \n",
       "2                     0.6629            NaN             ...               \n",
       "3                     0.7039            NaN             ...               \n",
       "4                     1.0000            NaN             ...               \n",
       "\n",
       "  relevant_yn_gold retweet_count  sentiment_gold subject_matter_gold  \\\n",
       "0              NaN             5             NaN                 NaN   \n",
       "1              NaN            26             NaN                 NaN   \n",
       "2              NaN            27             NaN                 NaN   \n",
       "3              NaN           138             NaN                 NaN   \n",
       "4              NaN           156             NaN                 NaN   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...         NaN   \n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...         NaN   \n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...         NaN   \n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...         NaN   \n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...         NaN   \n",
       "\n",
       "               tweet_created            tweet_id  tweet_location  \\\n",
       "0  2015-08-07 09:54:46 -0700  629697200650592256             NaN   \n",
       "1  2015-08-07 09:54:46 -0700  629697199560069120             NaN   \n",
       "2  2015-08-07 09:54:46 -0700  629697199312482304             NaN   \n",
       "3  2015-08-07 09:54:45 -0700  629697197118861312           Texas   \n",
       "4  2015-08-07 09:54:45 -0700  629697196967903232             NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                       Quito  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3  Central Time (US & Canada)  \n",
       "4                     Arizona  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1111)\n",
    "\n",
    "df = pd.read_csv('data/Sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at some basic stats of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>8493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>3142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>2236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "sentiment      \n",
       "Negative   8493\n",
       "Neutral    3142\n",
       "Positive   2236"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.groupby('sentiment').count()['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For simplicity, \n",
    "- we only use ```X = 'sentiment'``` and ```y = 'text'``` from the original dataframe. \n",
    "- We only look at positive (1) and negative (0) tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "sentiment      \n",
       "0          8493\n",
       "1          2236"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['sentiment', 'text']]\n",
    "df = df[df['sentiment'] != 'Neutral']\n",
    "df['sentiment'] = [1 if s == \"Positive\" else 0 for s in df['sentiment']]\n",
    "df.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79.152858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.847142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text\n",
       "sentiment           \n",
       "0          79.152858\n",
       "1          20.847142"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.10, random_state=42)\n",
    "train_data.groupby('sentiment').count().apply(lambda x: 100 * x / float(x.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y = train_data['text'], train_data['sentiment']\n",
    "test_X, test_y = test_data['text'], test_data['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Input representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Build vocabulary\n",
    "We need to build a vocabulary using words in our training data. Any words in the test set that are not in our vocabulary will be replaced with an ```<UNK>``` token. We will also add a ```<PAD>``` token as padding.\n",
    "\n",
    "For computational purposes, we'll only take words that appeared more than 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3113\n"
     ]
    }
   ],
   "source": [
    "UNK = \"<UNK>\"\n",
    "PAD = \"<PAD>\"\n",
    "def build_vocab(sentences, min_count=3, max_vocab=None):\n",
    "    \"\"\"\n",
    "    Build vocabulary from sentences (list of strings)\n",
    "    \"\"\"\n",
    "    # keep track of the number of appearance of each word\n",
    "    word_count = Counter()\n",
    "    \n",
    "    for s in sentences:\n",
    "        word_count.update(re.findall(r\"[\\w']+|[.,!?;]\", s.lower()))\n",
    "    \n",
    "    vocabulary = list([w for w in word_count if word_count[w] > min_count]) + [UNK, PAD]\n",
    "    indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return vocabulary, indices\n",
    "    \n",
    "vocabulary, vocab_indices = build_vocab(train_X)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is great - let's have a bunch of rich entitled MEN make decisions about #PlannedParenthood #GOPDebates\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data is a string of words\n",
    "train_X.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Word representations\n",
    "Next, we neeed to convert each word/token in the sentences into its index in the vocabulary so that pytorch can use it. We also pad our sentences to a fixed length of 25 tokens so that we can do batch processing. We do this for both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_padded_index_sequences(words, sentences, pad_length=100):\n",
    "    padded_sequences = np.zeros((len(sentences), pad_length))\n",
    "    for i, s in enumerate(sentences):\n",
    "        indices = np.ones(pad_length) * words['<PAD>']\n",
    "        # only take the first pad_length tokens\n",
    "        token_indices = np.array([words[w] if w in words else words['<UNK>'] for w in re.findall(r\"[\\w']+|[.,!?;]\", s.lower())[:pad_length]])\n",
    "        indices[:len(token_indices)] = token_indices\n",
    "        padded_sequences[i] = indices\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X = sentences_to_padded_index_sequences(vocab_indices, train_data['text'], 25)\n",
    "test_X = sentences_to_padded_index_sequences(vocab_indices, test_data['text'], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   1.00000000e+00,   2.00000000e+00,\n",
       "         3.00000000e+00,   4.00000000e+00,   5.00000000e+00,\n",
       "         6.00000000e+00,   7.00000000e+00,   8.00000000e+00,\n",
       "         3.11100000e+03,   9.00000000e+00,   1.00000000e+01,\n",
       "         1.10000000e+01,   1.20000000e+01,   1.30000000e+01,\n",
       "         1.40000000e+01,   3.11200000e+03,   3.11200000e+03,\n",
       "         3.11200000e+03,   3.11200000e+03,   3.11200000e+03,\n",
       "         3.11200000e+03,   3.11200000e+03,   3.11200000e+03,\n",
       "         3.11200000e+03])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data is now an array of indices\n",
    "train_X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences.astype(int)\n",
    "        self.labels = np.array(labels).astype(int)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return (torch.LongTensor(self.sentences[key]), self.labels[key])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(TweetDataset(train_X, train_y),\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(TweetDataset(test_X, test_y),\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Train and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader=train_loader, test_loader=test_loader, \n",
    "          learning_rate=0.001, num_epoch=10, print_every=100):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epoch):\n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "            outputs = model(data)\n",
    "            model.zero_grad()\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "             # report performance\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('Train set | epoch: {:3d} | {:6d}/{:6d} batches | Loss: {:6.4f}'.format(\n",
    "                    epoch, i + 1, len(train_loader), loss.item()))     \n",
    "#                 print('Epoch: [{0}/{1}], Step: [{2}/{3}], Loss: {4}, Validation Acc:{5}, AUC:{6}'.format(\n",
    "#                     epoch + 1, EPOCHS, i + 1, len(train_loader), loss.data[0], test_acc, test_auc))\n",
    "    \n",
    "    # Evaluate after every epochh\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        truths = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, labels) in enumerate(test_loader):\n",
    "                outputs = model(data).squeeze()\n",
    "#                 import ipdb; ipdb.set_trace()\n",
    "#                 predicted = ((outputs > 0.5).long()).view(-1)\n",
    "                pred = outputs.data.max(1)[1]\n",
    "                predictions += list(pred.numpy())\n",
    "                truths += list(labels.numpy())\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum()\n",
    "                \n",
    "            acc = (100 * correct / total)\n",
    "            auc = roc_auc_score(truths, predictions)\n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "            print('Test set | Accuracy: {:6.4f} | AUC: {:4.2f} | time elapse: {:>9}'.format(\n",
    "                acc, auc, elapse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For this lab, we will be exploring two variants of RNN: vanilla (or Elman) RNN and LSTM (Long-short term memory). In the following code block, please try to define your own model. Here are some hints.\n",
    "\n",
    "- Each input word is represented by a vector of dimension ```embedding_dim```. Check out ```nn.Embedding``` to see how to initialize embeddings randomly.\n",
    "- Your model should take the following input parameters\n",
    "    - ```hidden_dim```: The number of features in the hidden state h of your RNN layer\n",
    "    - ```output_dim```: Number of output classes\n",
    "    - ```vocab_size``` Size of your vocabulary. \n",
    "    - ```embedding_dim```: Dimension of word embeddings\n",
    "- Your model should consist of an RNN layer (you can use either ```nn.RNN``` or ```nn.LSTM```) followed by a linear layer.\n",
    "- $h_{0}$ (and $c$ if you use LSTM) should be initialized as a zero vector of dimension ```hidden_dim```. You might want to check out ```nn.Parameter```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, \n",
    "                 vocab_size, embedding_dim, rnn='LSTM'):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_size-1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn_fn = rnn\n",
    "        assert self.rnn_fn in ['LSTM', 'RNN']\n",
    "        self.rnn = getattr(nn, rnn)(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state values\n",
    "        \"\"\"\n",
    "        hidden = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            c = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "            return hidden, c\n",
    "        return hidden\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        _, last_hidden = self.rnn(x, self.init_hidden(x.size()[0]))\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            last_hidden = last_hidden[0]\n",
    "        out = self.fc(last_hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Run the code block below to check your model performance. Using the parameters provided, you should be able to get about 0.6 AUC using vanilla RNN or about 0.7 AUC using LSTM after 10 training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set | epoch:   0 |    100/   302 batches | Loss: 0.5581\n",
      "Train set | epoch:   0 |    200/   302 batches | Loss: 0.3743\n",
      "Train set | epoch:   0 |    300/   302 batches | Loss: 0.3921\n",
      "Test set | Accuracy: 79.0000 | AUC: 0.52 | time elapse:  00:00:03\n",
      "Train set | epoch:   1 |    100/   302 batches | Loss: 0.4053\n",
      "Train set | epoch:   1 |    200/   302 batches | Loss: 0.3868\n",
      "Train set | epoch:   1 |    300/   302 batches | Loss: 0.3857\n",
      "Test set | Accuracy: 79.0000 | AUC: 0.52 | time elapse:  00:00:06\n",
      "Train set | epoch:   2 |    100/   302 batches | Loss: 0.3922\n",
      "Train set | epoch:   2 |    200/   302 batches | Loss: 0.3411\n",
      "Train set | epoch:   2 |    300/   302 batches | Loss: 0.4422\n",
      "Test set | Accuracy: 79.0000 | AUC: 0.52 | time elapse:  00:00:09\n",
      "Train set | epoch:   3 |    100/   302 batches | Loss: 0.5017\n",
      "Train set | epoch:   3 |    200/   302 batches | Loss: 0.2832\n",
      "Train set | epoch:   3 |    300/   302 batches | Loss: 0.3281\n",
      "Test set | Accuracy: 80.0000 | AUC: 0.55 | time elapse:  00:00:13\n",
      "Train set | epoch:   4 |    100/   302 batches | Loss: 0.3366\n",
      "Train set | epoch:   4 |    200/   302 batches | Loss: 0.4139\n",
      "Train set | epoch:   4 |    300/   302 batches | Loss: 0.4188\n",
      "Test set | Accuracy: 81.0000 | AUC: 0.59 | time elapse:  00:00:16\n",
      "Train set | epoch:   5 |    100/   302 batches | Loss: 0.3942\n",
      "Train set | epoch:   5 |    200/   302 batches | Loss: 0.3688\n",
      "Train set | epoch:   5 |    300/   302 batches | Loss: 0.3912\n",
      "Test set | Accuracy: 81.0000 | AUC: 0.58 | time elapse:  00:00:19\n",
      "Train set | epoch:   6 |    100/   302 batches | Loss: 0.4626\n",
      "Train set | epoch:   6 |    200/   302 batches | Loss: 0.2160\n",
      "Train set | epoch:   6 |    300/   302 batches | Loss: 0.3289\n",
      "Test set | Accuracy: 80.0000 | AUC: 0.59 | time elapse:  00:00:22\n",
      "Train set | epoch:   7 |    100/   302 batches | Loss: 0.1909\n",
      "Train set | epoch:   7 |    200/   302 batches | Loss: 0.5012\n",
      "Train set | epoch:   7 |    300/   302 batches | Loss: 0.3811\n",
      "Test set | Accuracy: 79.0000 | AUC: 0.58 | time elapse:  00:00:26\n",
      "Train set | epoch:   8 |    100/   302 batches | Loss: 0.3261\n",
      "Train set | epoch:   8 |    200/   302 batches | Loss: 0.1930\n",
      "Train set | epoch:   8 |    300/   302 batches | Loss: 0.5670\n",
      "Test set | Accuracy: 77.0000 | AUC: 0.60 | time elapse:  00:00:29\n",
      "Train set | epoch:   9 |    100/   302 batches | Loss: 0.3170\n",
      "Train set | epoch:   9 |    200/   302 batches | Loss: 0.5070\n",
      "Train set | epoch:   9 |    300/   302 batches | Loss: 0.3231\n",
      "Test set | Accuracy: 77.0000 | AUC: 0.60 | time elapse:  00:00:32\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "rnn_model = RNN(40, 2, len(vocabulary), 50, rnn='RNN')\n",
    "train(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set | epoch:   0 |    100/   302 batches | Loss: 0.4903\n",
      "Train set | epoch:   0 |    200/   302 batches | Loss: 0.6375\n",
      "Train set | epoch:   0 |    300/   302 batches | Loss: 0.3719\n",
      "Test set | Accuracy: 79.0000 | AUC: 0.51 | time elapse:  00:00:05\n",
      "Train set | epoch:   1 |    100/   302 batches | Loss: 0.6755\n",
      "Train set | epoch:   1 |    200/   302 batches | Loss: 0.4191\n",
      "Train set | epoch:   1 |    300/   302 batches | Loss: 0.4653\n",
      "Test set | Accuracy: 82.0000 | AUC: 0.62 | time elapse:  00:00:10\n",
      "Train set | epoch:   2 |    100/   302 batches | Loss: 0.5240\n",
      "Train set | epoch:   2 |    200/   302 batches | Loss: 0.3050\n",
      "Train set | epoch:   2 |    300/   302 batches | Loss: 0.4367\n",
      "Test set | Accuracy: 82.0000 | AUC: 0.66 | time elapse:  00:00:15\n",
      "Train set | epoch:   3 |    100/   302 batches | Loss: 0.2876\n",
      "Train set | epoch:   3 |    200/   302 batches | Loss: 0.3122\n",
      "Train set | epoch:   3 |    300/   302 batches | Loss: 0.2083\n",
      "Test set | Accuracy: 82.0000 | AUC: 0.71 | time elapse:  00:00:21\n",
      "Train set | epoch:   4 |    100/   302 batches | Loss: 0.3285\n",
      "Train set | epoch:   4 |    200/   302 batches | Loss: 0.1430\n",
      "Train set | epoch:   4 |    300/   302 batches | Loss: 0.2330\n",
      "Test set | Accuracy: 82.0000 | AUC: 0.70 | time elapse:  00:00:26\n",
      "Train set | epoch:   5 |    100/   302 batches | Loss: 0.5068\n",
      "Train set | epoch:   5 |    200/   302 batches | Loss: 0.4023\n",
      "Train set | epoch:   5 |    300/   302 batches | Loss: 0.3881\n",
      "Test set | Accuracy: 83.0000 | AUC: 0.67 | time elapse:  00:00:31\n",
      "Train set | epoch:   6 |    100/   302 batches | Loss: 0.2245\n",
      "Train set | epoch:   6 |    200/   302 batches | Loss: 0.2627\n",
      "Train set | epoch:   6 |    300/   302 batches | Loss: 0.2935\n",
      "Test set | Accuracy: 82.0000 | AUC: 0.71 | time elapse:  00:00:37\n",
      "Train set | epoch:   7 |    100/   302 batches | Loss: 0.1247\n",
      "Train set | epoch:   7 |    200/   302 batches | Loss: 0.2459\n",
      "Train set | epoch:   7 |    300/   302 batches | Loss: 0.2073\n",
      "Test set | Accuracy: 83.0000 | AUC: 0.69 | time elapse:  00:00:43\n",
      "Train set | epoch:   8 |    100/   302 batches | Loss: 0.2837\n",
      "Train set | epoch:   8 |    200/   302 batches | Loss: 0.2409\n",
      "Train set | epoch:   8 |    300/   302 batches | Loss: 0.1958\n",
      "Test set | Accuracy: 82.0000 | AUC: 0.69 | time elapse:  00:00:48\n",
      "Train set | epoch:   9 |    100/   302 batches | Loss: 0.1077\n",
      "Train set | epoch:   9 |    200/   302 batches | Loss: 0.1858\n",
      "Train set | epoch:   9 |    300/   302 batches | Loss: 0.0800\n",
      "Test set | Accuracy: 81.0000 | AUC: 0.70 | time elapse:  00:00:54\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "lstm_model = RNN(40, 2, len(vocabulary), 50, rnn='LSTM')\n",
    "train(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_sentence(sentence, model):\n",
    "    model.eval()\n",
    "    test_tensor = torch.LongTensor(sentences_to_padded_index_sequences(vocab_indices, [sentence]).astype(int))\n",
    "    score = model(test_tensor).data.numpy().squeeze()\n",
    "    label = np.argmax(score)\n",
    "    \n",
    "    return (\"positive\" if label == 1 else \"negative\", score[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('positive', 1.3321191)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence(\"Enjoyed the #GOPDebates and am looking forward to the #DemocraticDebates next.\", lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', 1.9322957)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence(\"Donald Trump is a really nasty piece of work. Hope he disappears quickly. #GOPDebate\", lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Word Embeddings and How to Use Them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "When using deep learning methods on NLP tasks, we usually utilize [word embedding](https://en.wikipedia.org/wiki/Word_embedding). To put it briefly, word embedding represent words, or tokens, in a vocabulary as a distributed numerical vector. There are a lot of methods to obtain a word embedding, with some of the most famous being Word2Vec, GloVe, and ELMo. It is not difficult to find a general purpose word embedding trained by one of the aforementioned methods on the Internet that's been trained with a massive amount of data. It is usually a good idea to use these pre-trained embedding to save yourself some time and computing resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this lab, we will be using the [GloVe embedding](https://nlp.stanford.edu/projects/glove/) developed by Stanford,  one of the state-of-the-art word embedding. Please download the file ```glove.6B.50d.txt``` [here](https://drive.google.com/file/d/1JweINiA5JvTNLTm663LH8OdWssK2Kcid/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:06, 62159.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# load embedding\n",
    "emb_dim = 50\n",
    "with open('glove.6B/glove.6B.50d.txt') as f:\n",
    "    glove_embedding = []\n",
    "    words = {}\n",
    "    chars = {}\n",
    "    idx2words = {}\n",
    "    ordered_words = []\n",
    "\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        s = line.split()\n",
    "        glove_embedding.append(np.asarray(s[1:]))\n",
    "        \n",
    "        words[s[0]] = len(words)\n",
    "        idx2words[i] = s[0]\n",
    "        ordered_words.append(s[0])\n",
    "        \n",
    "# add unknown to word and char\n",
    "glove_embedding.append(np.random.rand(emb_dim))\n",
    "words[\"<UNK>\"] = len(words)\n",
    "\n",
    "# add padding\n",
    "glove_embedding.append(np.zeros(emb_dim))\n",
    "words[\"<PAD>\"] = len(words)\n",
    "\n",
    "chars[\"<UNK>\"] = len(chars)\n",
    "chars[\"<PAD>\"] = len(chars)\n",
    "\n",
    "glove_embedding = np.array(glove_embedding).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we have three variables\n",
    "- ```glove_embedding``` of shape [106687, 50] consisting of the actual vectors,\n",
    "- ```words```, a dictionary consisting of each token in the vocabulary and its corresponding row in ```glove_embedding```, and\n",
    "- ```idx2words```, a list consisting of all the words in their order in ```glove_embedding```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we can play around with these vectors to get a sense of how word embeddings can be used to represent words. Here's how you can look up a word embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.30740000e-01,   4.01170000e-01,  -4.07850000e-01,\n",
       "         1.54440000e-01,   4.77820000e-01,   2.07540000e-01,\n",
       "        -2.69510000e-01,  -3.40230000e-01,  -1.08790000e-01,\n",
       "         1.05630000e-01,  -1.02890000e-01,   1.08490000e-01,\n",
       "        -4.96810000e-01,  -2.51280000e-01,   8.40250000e-01,\n",
       "         3.89490000e-01,   3.22840000e-01,  -2.27970000e-01,\n",
       "        -4.43420000e-01,  -3.16490000e-01,  -1.24060000e-01,\n",
       "        -2.81700000e-01,   1.94670000e-01,   5.55130000e-02,\n",
       "         5.67050000e-01,  -1.74190000e+00,  -9.11450000e-01,\n",
       "         2.70360000e-01,   4.19270000e-01,   2.02790000e-02,\n",
       "         4.04050000e+00,  -2.49430000e-01,  -2.04160000e-01,\n",
       "        -6.27620000e-01,  -5.47830000e-02,  -2.68830000e-01,\n",
       "         1.84440000e-01,   1.82040000e-01,  -2.35360000e-01,\n",
       "        -1.61550000e-01,  -2.76550000e-01,   3.55060000e-02,\n",
       "        -3.82110000e-01,  -7.51340000e-04,  -2.48220000e-01,\n",
       "         2.81640000e-01,   1.28190000e-01,   2.87620000e-01,\n",
       "         1.44400000e-01,   2.36110000e-01])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding[words['this']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar words\n",
    "\n",
    "The word embedding vectors can help us find words with similar meanings. Word similarities can be measured by [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). The function below looks up the most similar words to a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_nearest(ref_vec, words, embedding, topk=10):\n",
    "    \"\"\"\n",
    "    Finds the top-k most similar words to \"word\" in terms of cosine similarity in the given embedding\n",
    "    :param ref_vec: reference word vector\n",
    "    :param words: dict, word to its index in the embedding\n",
    "    :param embedding: numpy array of shape [V, embedding_dim]\n",
    "    :param topk: number of top candidates to return\n",
    "    :return a list of top-k most similar words\n",
    "    \"\"\"\n",
    "    # compute cosine similarities\n",
    "    scored_words = cosine_similarity(ref_vec.reshape(1,-1), loaded_embeddings)[0]\n",
    "    \n",
    "    # sort the words by similarity and return the topk\n",
    "    sorted_words = np.argsort(-scored_words)\n",
    "    \n",
    "    return [(idx2words[w], scored_words[w]) for w in sorted_words[:topk]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hate', 0.99999999999999978),\n",
       " ('hatred', 0.77468372337488278),\n",
       " ('shame', 0.74895365817045212),\n",
       " ('racist', 0.73715591114403145),\n",
       " ('anyone', 0.73647167276271064)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest(glove_embedding[words['hate']], words, glove_embedding, topk=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 0.81096602138267371)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest(loaded_embeddings[words['worse']] - loaded_embeddings[words['better']] + loaded_embeddings[words['best']],\n",
    "            words, loaded_embeddings, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.87060674388747061)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest(loaded_embeddings[words['king']] - loaded_embeddings[words['queen']] + loaded_embeddings[words['woman']],\n",
    "            words, loaded_embeddings, topk=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an LSTM model withh GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Complete the code below. Replace the randomly generated embeddings withh GloVe embeddings. (Hint: check out ```nn.Embedding.weight```). Using the parameters provided, you should be able to get about 0.75 AUC using GloVe embeddings after 10 training epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-indexing tokens\n",
    "train_X_glove = sentences_to_padded_index_sequences(words, train_data['text'], 25)\n",
    "test_X_glove = sentences_to_padded_index_sequences(words, test_data['text'], 25)\n",
    "\n",
    "train_loader_glove = DataLoader(TweetDataset(train_X_glove, train_y),\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True)\n",
    "test_loader_glove = DataLoader(TweetDataset(test_X_glove, test_y),\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set | epoch:   0 |    100/   302 batches | Loss: 0.4807\n",
      "Train set | epoch:   0 |    200/   302 batches | Loss: 0.3671\n",
      "Train set | epoch:   0 |    300/   302 batches | Loss: 0.6753\n",
      "Test set | Accuracy: 80.0000 | AUC: 0.53 | time elapse:  00:00:57\n",
      "Train set | epoch:   1 |    100/   302 batches | Loss: 0.6563\n",
      "Train set | epoch:   1 |    200/   302 batches | Loss: 0.2139\n",
      "Train set | epoch:   1 |    300/   302 batches | Loss: 0.3274\n",
      "Test set | Accuracy: 80.0000 | AUC: 0.75 | time elapse:  00:01:53\n",
      "Train set | epoch:   2 |    100/   302 batches | Loss: 0.2582\n",
      "Train set | epoch:   2 |    200/   302 batches | Loss: 0.1890\n",
      "Train set | epoch:   2 |    300/   302 batches | Loss: 0.2146\n",
      "Test set | Accuracy: 84.0000 | AUC: 0.69 | time elapse:  00:02:49\n",
      "Train set | epoch:   3 |    100/   302 batches | Loss: 0.1729\n",
      "Train set | epoch:   3 |    200/   302 batches | Loss: 0.1464\n",
      "Train set | epoch:   3 |    300/   302 batches | Loss: 0.2360\n",
      "Test set | Accuracy: 84.0000 | AUC: 0.76 | time elapse:  00:03:47\n",
      "Train set | epoch:   4 |    100/   302 batches | Loss: 0.3925\n",
      "Train set | epoch:   4 |    200/   302 batches | Loss: 0.1854\n",
      "Train set | epoch:   4 |    300/   302 batches | Loss: 0.2337\n",
      "Test set | Accuracy: 85.0000 | AUC: 0.73 | time elapse:  00:04:44\n",
      "Train set | epoch:   5 |    100/   302 batches | Loss: 0.0717\n",
      "Train set | epoch:   5 |    200/   302 batches | Loss: 0.1966\n",
      "Train set | epoch:   5 |    300/   302 batches | Loss: 0.1084\n",
      "Test set | Accuracy: 84.0000 | AUC: 0.71 | time elapse:  00:05:44\n",
      "Train set | epoch:   6 |    100/   302 batches | Loss: 0.1920\n",
      "Train set | epoch:   6 |    200/   302 batches | Loss: 0.2225\n",
      "Train set | epoch:   6 |    300/   302 batches | Loss: 0.1222\n",
      "Test set | Accuracy: 84.0000 | AUC: 0.74 | time elapse:  00:06:41\n",
      "Train set | epoch:   7 |    100/   302 batches | Loss: 0.0967\n",
      "Train set | epoch:   7 |    200/   302 batches | Loss: 0.0591\n",
      "Train set | epoch:   7 |    300/   302 batches | Loss: 0.1991\n",
      "Test set | Accuracy: 84.0000 | AUC: 0.73 | time elapse:  00:07:38\n",
      "Train set | epoch:   8 |    100/   302 batches | Loss: 0.0508\n",
      "Train set | epoch:   8 |    200/   302 batches | Loss: 0.0184\n",
      "Train set | epoch:   8 |    300/   302 batches | Loss: 0.0496\n",
      "Test set | Accuracy: 84.0000 | AUC: 0.73 | time elapse:  00:08:34\n",
      "Train set | epoch:   9 |    100/   302 batches | Loss: 0.2283\n",
      "Train set | epoch:   9 |    200/   302 batches | Loss: 0.1212\n",
      "Train set | epoch:   9 |    300/   302 batches | Loss: 0.1033\n",
      "Test set | Accuracy: 83.0000 | AUC: 0.73 | time elapse:  00:09:30\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "glove_model = RNN(40, 2, len(glove_embedding), 50, rnn='LSTM')\n",
    "glove_model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "train(glove_model, train_loader=train_loader_glove, test_loader=test_loader_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
